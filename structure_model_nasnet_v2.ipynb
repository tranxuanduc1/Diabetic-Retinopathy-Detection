{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two_stage_nasnet_pipeline.py\n",
    "import os, math, json, random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.applications import NASNetLarge\n",
    "from tensorflow.keras.applications.nasnet import preprocess_input as nasnet_preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\tran1\\AppData\\Local\\Temp\\ipykernel_16288\\1344325219.py:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  TRAIN_DIR = \"D:\\Diux\\hoctap\\DoAn\\ddr\\\\train_preprocess\"        # chứa 5 thư mục con 0..4\n",
      "C:\\Users\\tran1\\AppData\\Local\\Temp\\ipykernel_16288\\1344325219.py:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  VAL_DIR   = \"D:\\Diux\\hoctap\\DoAn\\ddr\\\\val_preprocess\"          # chứa 5 thư mục con 0..4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base datasets...\n",
      "Found 20693 files belonging to 5 classes.\n",
      "Found 2504 files belonging to 5 classes.\n",
      "\n",
      "=== Module 1: No-DR vs DR ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<unknown>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<unknown>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<unknown>:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<unknown>:6: SyntaxWarning: invalid escape sequence '\\D'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/nasnet/NASNet-large-no-top.h5\n",
      "\u001b[1m 46006272/343610240\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m53:34\u001b[0m 11us/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tran1\\AppData\\Local\\Temp\\ipykernel_16288\\1344325219.py:5: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  TRAIN_DIR = \"D:\\Diux\\hoctap\\DoAn\\ddr\\\\train_preprocess\"        # chứa 5 thư mục con 0..4\n",
      "C:\\Users\\tran1\\AppData\\Local\\Temp\\ipykernel_16288\\1344325219.py:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  VAL_DIR   = \"D:\\Diux\\hoctap\\DoAn\\ddr\\\\val_preprocess\"          # chứa 5 thư mục con 0..4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 236\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;66;03m# =============== Main ===============\u001b[39;00m\n\u001b[32m    235\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     \u001b[43mtrain_two_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m     \u001b[38;5;66;03m# Ví dụ suy luận:\u001b[39;00m\n\u001b[32m    238\u001b[39m     \u001b[38;5;66;03m# label, info = predict_stage(\"some_image.jpg\")\u001b[39;00m\n\u001b[32m    239\u001b[39m     \u001b[38;5;66;03m# print(label, info)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 137\u001b[39m, in \u001b[36mtrain_two_stage\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    134\u001b[39m tr1 = make_pipeline_for_module1(train_base, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    135\u001b[39m va1 = make_pipeline_for_module1(val_base,   training=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m m1, m1_base = \u001b[43mbuild_module1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m cbs1 = [\n\u001b[32m    140\u001b[39m     EarlyStopping(patience=\u001b[32m5\u001b[39m, restore_best_weights=\u001b[38;5;28;01mTrue\u001b[39;00m, monitor=\u001b[33m'\u001b[39m\u001b[33mval_auc\u001b[39m\u001b[33m'\u001b[39m, mode=\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    141\u001b[39m     ReduceLROnPlateau(patience=\u001b[32m2\u001b[39m, factor=\u001b[32m0.5\u001b[39m, min_lr=\u001b[32m1e-6\u001b[39m, monitor=\u001b[33m'\u001b[39m\u001b[33mval_auc\u001b[39m\u001b[33m'\u001b[39m, mode=\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m    142\u001b[39m     ModelCheckpoint(os.path.join(OUT_DIR, \u001b[33m\"\u001b[39m\u001b[33mmodule1_best.keras\u001b[39m\u001b[33m\"\u001b[39m), monitor=\u001b[33m'\u001b[39m\u001b[33mval_auc\u001b[39m\u001b[33m'\u001b[39m, mode=\u001b[33m'\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m'\u001b[39m, save_best_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    143\u001b[39m ]\n\u001b[32m    145\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStage-1 (head) ...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mbuild_module1\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_module1\u001b[39m():\n\u001b[32m     96\u001b[39m     \u001b[38;5;66;03m# Binary: No-DR (0) vs DR (1)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     inp, feat, base = \u001b[43mbuild_backbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m     x = layers.Dense(\u001b[32m512\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m)(feat)\n\u001b[32m     99\u001b[39m     x = layers.Dropout(\u001b[32m0.3\u001b[39m)(x)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mbuild_backbone\u001b[39m\u001b[34m(trainable)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_backbone\u001b[39m(trainable=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     base = \u001b[43mNASNetLarge\u001b[49m\u001b[43m(\u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mavg\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimagenet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m     base.trainable = trainable\n\u001b[32m     90\u001b[39m     inp  = layers.Input(shape=(IMG_SIZE, IMG_SIZE, \u001b[32m3\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Diux\\hoctap\\DoAn\\myvenv\\Lib\\site-packages\\keras\\src\\applications\\nasnet.py:481\u001b[39m, in \u001b[36mNASNetLarge\u001b[39m\u001b[34m(input_shape, include_top, weights, input_tensor, pooling, classes, classifier_activation, name)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;129m@keras_export\u001b[39m(\n\u001b[32m    412\u001b[39m     [\n\u001b[32m    413\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mkeras.applications.nasnet.NASNetLarge\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    425\u001b[39m     name=\u001b[33m\"\u001b[39m\u001b[33mnasnet_large\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    426\u001b[39m ):\n\u001b[32m    427\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Instantiates a NASNet model in ImageNet mode.\u001b[39;00m\n\u001b[32m    428\u001b[39m \n\u001b[32m    429\u001b[39m \u001b[33;03m    Reference:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    479\u001b[39m \u001b[33;03m        A Keras model instance.\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m481\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNASNet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenultimate_filters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4032\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstem_block_filters\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m96\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip_reduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilter_multiplier\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude_top\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdefault_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m331\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclassifier_activation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Diux\\hoctap\\DoAn\\myvenv\\Lib\\site-packages\\keras\\src\\applications\\nasnet.py:297\u001b[39m, in \u001b[36mNASNet\u001b[39m\u001b[34m(input_shape, penultimate_filters, num_blocks, stem_block_filters, skip_reduction, filter_multiplier, include_top, weights, input_tensor, pooling, classes, default_size, classifier_activation, name)\u001b[39m\n\u001b[32m    290\u001b[39m         weights_path = file_utils.get_file(\n\u001b[32m    291\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnasnet_large.h5\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    292\u001b[39m             NASNET_LARGE_WEIGHT_PATH,\n\u001b[32m    293\u001b[39m             cache_subdir=\u001b[33m\"\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    294\u001b[39m             file_hash=\u001b[33m\"\u001b[39m\u001b[33m11577c9a518f0070763c2b964a382f17\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    295\u001b[39m         )\n\u001b[32m    296\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m         weights_path = \u001b[43mfile_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnasnet_large_no_top.h5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m            \u001b[49m\u001b[43mNASNET_LARGE_WEIGHT_PATH_NO_TOP\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_subdir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfile_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43md81d89dc07e6e56530c4e77faddd61b5\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m     model.load_weights(weights_path)\n\u001b[32m    304\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Diux\\hoctap\\DoAn\\myvenv\\Lib\\site-packages\\keras\\src\\utils\\file_utils.py:311\u001b[39m, in \u001b[36mget_file\u001b[39m\u001b[34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir, force_download)\u001b[39m\n\u001b[32m    309\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m         \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDLProgbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m urllib.error.HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    313\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(error_msg.format(origin, e.code, e.msg))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\urllib\\request.py:268\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reporthook:\n\u001b[32m    266\u001b[39m     reporthook(blocknum, bs, size)\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m block := \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    269\u001b[39m     read += \u001b[38;5;28mlen\u001b[39m(block)\n\u001b[32m    270\u001b[39m     tfp.write(block)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:472\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    470\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    471\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    474\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    475\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1249\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1245\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1246\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1247\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1248\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1107\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# =============== Config ===============\n",
    "SEED = 42\n",
    "random.seed(SEED); tf.random.set_seed(SEED)\n",
    "\n",
    "TRAIN_DIR = \"D:\\Diux\\hoctap\\DoAn\\ddr\\\\train_preprocess\"        \n",
    "VAL_DIR   = \"D:\\Diux\\hoctap\\DoAn\\ddr\\\\val_preprocess\"          \n",
    "\n",
    "IMG_SIZE  = 331                       # chuẩn NASNetLarge\n",
    "BATCH     = 8\n",
    "\n",
    "# Warmup + fine-tune\n",
    "HEAD_EPOCHS = 5                       # train head (đóng băng backbone)\n",
    "FT_EPOCHS   = 5                      # fine-tune toàn bộ\n",
    "LR_HEAD     = 3e-4\n",
    "LR_FT       = 1e-4\n",
    "\n",
    "OUT_DIR     = \"outputs_two_stage\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============== Dataset loaders ===============\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def make_base_ds(root_dir, subset=\"train\"):\n",
    "    \"\"\"\n",
    "    Load DS 5 lớp 0..4 từ folder. Không augment ở đây (augment sẽ ở pipeline).\n",
    "    \"\"\"\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        root_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        class_names=['0','1','2','3','4'],\n",
    "        image_size=(IMG_SIZE, IMG_SIZE),\n",
    "        batch_size=BATCH,\n",
    "        shuffle=True if subset==\"train\" else False,\n",
    "        seed=SEED\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "\n",
    "def preprocess(x):\n",
    "    # x: float32 [0,255]\n",
    "    return nasnet_preprocess(x)\n",
    "\n",
    "def make_pipeline_for_module1(ds, training=True):\n",
    "    \"\"\"\n",
    "    Module 1: No-DR (y=0) vs DR (y in 1..4)  -> nhãn nhị phân {0,1}\n",
    "    \"\"\"\n",
    "    def map_to_bin(x, y):\n",
    "        y_bin = tf.where(tf.equal(y, 0), tf.zeros_like(y), tf.ones_like(y))\n",
    "        return x, tf.cast(y_bin, tf.float32)\n",
    "\n",
    "    ds = ds.map(map_to_bin, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32), y), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(lambda x,y: (preprocess(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "def make_pipeline_for_module2(ds, training=True):\n",
    "    \"\"\"\n",
    "    Module 2: chỉ giữ mẫu DR (y in 1..4), ánh xạ nhãn 1..4 -> 0..3 (softmax 4 lớp)\n",
    "    \"\"\"\n",
    "    def filter_dr(x, y):\n",
    "        keep = tf.not_equal(y, 0)\n",
    "        return keep\n",
    "\n",
    "    def map_to_4(x, y):\n",
    "        y4 = y - 1  # 1..4 -> 0..3\n",
    "        return x, tf.cast(y4, tf.int32)\n",
    "\n",
    "    ds = ds.filter(filter_dr)\n",
    "    ds = ds.map(map_to_4, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32), y), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.map(lambda x,y: (preprocess(x), y), num_parallel_calls=AUTOTUNE)\n",
    "    return ds.prefetch(AUTOTUNE)\n",
    "\n",
    "# =============== Models ===============\n",
    "def build_backbone(trainable=False):\n",
    "    base = NASNetLarge(include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='avg', weights='imagenet')\n",
    "    base.trainable = trainable\n",
    "    inp  = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x    = base(inp, training=False)\n",
    "    x    = layers.Dropout(0.3)(x)\n",
    "    return inp, x, base\n",
    "\n",
    "def build_module1():\n",
    "    # Binary: No-DR (0) vs DR (1)\n",
    "    inp, feat, base = build_backbone(trainable=False)\n",
    "    x = layers.Dense(512, activation='relu')(feat)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(1, activation='sigmoid', name=\"bin_out\")(x)\n",
    "    model = models.Model(inp, out, name=\"NASNetL_Module1_NoDR_vs_DR\")\n",
    "    opt = tf.keras.optimizers.Adam(LR_HEAD)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model, base\n",
    "\n",
    "def build_module2():\n",
    "    # 4-class: 1..4 -> 0..3\n",
    "    inp, feat, base = build_backbone(trainable=False)\n",
    "    x = layers.Dense(512, activation='relu')(feat)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    out = layers.Dense(4, activation='softmax', name=\"stage4_out\")(x)\n",
    "    model = models.Model(inp, out, name=\"NASNetL_Module2_Stages_1to4\")\n",
    "    opt = tf.keras.optimizers.Adam(LR_HEAD)\n",
    "    model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model, base\n",
    "\n",
    "def unfreeze_all_and_recompile(model, base, lr=LR_FT):\n",
    "    base.trainable = True\n",
    "    # for layer in base.layers[:-50]: layer.trainable = False\n",
    "    opt = tf.keras.optimizers.Adam(lr)\n",
    "    # loss/metrics giữ nguyên\n",
    "    model.compile(optimizer=opt, loss=model.loss, metrics=model.metrics)\n",
    "    return model\n",
    "\n",
    "# =============== Training ===============\n",
    "def train_two_stage():\n",
    "    print(\"Loading base datasets...\")\n",
    "    train_base = make_base_ds(TRAIN_DIR, subset=\"train\")\n",
    "    val_base   = make_base_ds(VAL_DIR, subset=\"val\")\n",
    "\n",
    "    m1_best = os.path.join(OUT_DIR, \"module1_ft_best.keras\")\n",
    "    m2_best = os.path.join(OUT_DIR, \"module2_ft_best.keras\")\n",
    "\n",
    "    # ---------- Module 1 ----------\n",
    "    if os.path.exists(m1_best):\n",
    "        print(\"\\n=== Module 1: SKIP (đã có module1_ft_best.keras) ===\")\n",
    "        m1 = tf.keras.models.load_model(m1_best, compile=False)\n",
    "    else:\n",
    "        print(\"\\n=== Module 1: No-DR vs DR ===\")\n",
    "        tr1 = make_pipeline_for_module1(train_base, training=False)\n",
    "        va1 = make_pipeline_for_module1(val_base,   training=False)\n",
    "\n",
    "        m1, m1_base = build_module1()\n",
    "\n",
    "        cbs1 = [\n",
    "            EarlyStopping(patience=5, restore_best_weights=True, monitor='val_accuracy', mode='max'),\n",
    "            ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1e-6, monitor='val_accuracy', mode='max'),\n",
    "            ModelCheckpoint(m1_best, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "        ]\n",
    "\n",
    "        print(\"Stage-1 (head) ...\")\n",
    "        m1.fit(tr1, epochs=HEAD_EPOCHS, validation_data=va1, callbacks=cbs1, verbose=1)\n",
    "\n",
    "        print(\"Fine-tune all layers ...\")\n",
    "        m1 = unfreeze_all_and_recompile(m1, m1_base, lr=LR_FT)\n",
    "        cbs1_ft = [\n",
    "            EarlyStopping(patience=5, restore_best_weights=True, monitor='val_accuracy', mode='max'),\n",
    "            ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1e-7, monitor='val_accuracy', mode='max'),\n",
    "            ModelCheckpoint(m1_best, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "        ]\n",
    "        m1.fit(tr1, epochs=FT_EPOCHS, validation_data=va1, callbacks=cbs1_ft, verbose=1)\n",
    "        m1.save(os.path.join(OUT_DIR, \"module1_final.keras\"))\n",
    "        print(\"Saved Module 1.\")\n",
    "\n",
    "    # ---------- Module 2 ----------\n",
    "    if os.path.exists(m2_best):\n",
    "        print(\"\\n=== Module 2: SKIP (đã có module2_ft_best.keras) ===\")\n",
    "        m2 = tf.keras.models.load_model(m2_best, compile=False)\n",
    "    else:\n",
    "        print(\"\\n=== Module 2: Stages 1..4 (4-class) ===\")\n",
    "        tr2 = make_pipeline_for_module2(train_base, training=False)\n",
    "        va2 = make_pipeline_for_module2(val_base,   training=False)\n",
    "\n",
    "        m2, m2_base = build_module2()\n",
    "\n",
    "        cbs2 = [\n",
    "            EarlyStopping(patience=5, restore_best_weights=True, monitor='val_accuracy', mode='max'),\n",
    "            ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1e-6, monitor='val_accuracy', mode='max'),\n",
    "            ModelCheckpoint(m2_best, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "        ]\n",
    "\n",
    "        print(\"Stage-1 (head) ...\")\n",
    "        m2.fit(tr2, epochs=HEAD_EPOCHS, validation_data=va2, callbacks=cbs2, verbose=1)\n",
    "\n",
    "        print(\"Fine-tune all layers ...\")\n",
    "        m2 = unfreeze_all_and_recompile(m2, m2_base, lr=LR_FT)\n",
    "        cbs2_ft = [\n",
    "            EarlyStopping(patience=5, restore_best_weights=True, monitor='val_accuracy', mode='max'),\n",
    "            ReduceLROnPlateau(patience=2, factor=0.5, min_lr=1e-7, monitor='val_accuracy', mode='max'),\n",
    "            ModelCheckpoint(m2_best, monitor='val_accuracy', mode='max', save_best_only=True)\n",
    "        ]\n",
    "        m2.fit(tr2, epochs=FT_EPOCHS, validation_data=va2, callbacks=cbs2_ft, verbose=1)\n",
    "        m2.save(os.path.join(OUT_DIR, \"module2_final.keras\"))\n",
    "        print(\"Saved Module 2.\")\n",
    "\n",
    "\n",
    "# =============== Inference (2-stage) ===============\n",
    "def load_models_for_infer():\n",
    "    m1 = tf.keras.models.load_model(os.path.join(OUT_DIR, \"module1_ft_best.keras\"), compile=False)\n",
    "    m2 = tf.keras.models.load_model(os.path.join(OUT_DIR, \"module2_ft_best.keras\"), compile=False)\n",
    "    return m1, m2\n",
    "\n",
    "def load_and_preprocess_image(img_path):\n",
    "    img = tf.keras.utils.load_img(img_path, target_size=(IMG_SIZE, IMG_SIZE))\n",
    "    x   = tf.keras.utils.img_to_array(img)\n",
    "    x   = tf.cast(x, tf.float32)\n",
    "    x   = nasnet_preprocess(x)\n",
    "    return tf.expand_dims(x, 0)\n",
    "\n",
    "def predict_stage(img_path, thr=0.5):\n",
    "    \"\"\"\n",
    "    2-stage suy luận:\n",
    "     - Module1 -> pDR = sigmoid(out). Nếu pDR < thr => dự đoán lớp 0 (No-DR).\n",
    "     - Ngược lại -> Module2 (softmax 4 lớp 0..3) -> map về 1..4.\n",
    "    \"\"\"\n",
    "    m1, m2 = load_models_for_infer()\n",
    "    x = load_and_preprocess_image(img_path)\n",
    "\n",
    "    p_dr = float(m1.predict(x, verbose=0)[0][0])\n",
    "    if p_dr < thr:\n",
    "        return 0, {\"p_dr\": p_dr, \"module2\": None}\n",
    "\n",
    "    # DR: gọi module 2\n",
    "    probs = m2.predict(x, verbose=0)[0]  # shape (4,)\n",
    "    cls_0_3 = int(tf.argmax(probs).numpy())\n",
    "    final_label = cls_0_3 + 1           # map về 1..4\n",
    "    return final_label, {\"p_dr\": p_dr, \"probs_1to4\": probs.tolist()}\n",
    "\n",
    "# =============== Main ===============\n",
    "\n",
    "train_two_stage()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1253 files belonging to 5 classes.\n",
      "Loaded TEST set: 1253 samples\n",
      "\n",
      "========== Module 1 (No-DR vs DR) ==========\n",
      "Accuracy:  0.6872\n",
      "Precision: 0.6342\n",
      "Recall:    0.8834\n",
      "F1-score:  0.7383\n",
      "\n",
      "Chi tiết theo lớp:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    No-DR(0)       0.81      0.49      0.61       627\n",
      "       DR(1)       0.63      0.88      0.74       626\n",
      "\n",
      "    accuracy                           0.69      1253\n",
      "   macro avg       0.72      0.69      0.67      1253\n",
      "weighted avg       0.72      0.69      0.67      1253\n",
      "\n",
      "\n",
      "========== Module 2 (Stages 1..4) ==========\n",
      "Accuracy:  0.6038\n",
      "Precision (macro): 0.4443\n",
      "Recall (macro):    0.4856\n",
      "F1-score (macro):  0.4532\n",
      "\n",
      "Chi tiết theo lớp (0..3 tương ứng 1..4):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  stage1(=1)       0.19      0.40      0.26        63\n",
      "  stage2(=2)       0.80      0.65      0.72       448\n",
      "  stage3(=3)       0.25      0.29      0.27        24\n",
      "  stage4(=4)       0.54      0.60      0.57        91\n",
      "\n",
      "    accuracy                           0.60       626\n",
      "   macro avg       0.44      0.49      0.45       626\n",
      "weighted avg       0.68      0.60      0.63       626\n",
      "\n",
      "\n",
      "========== Two-Stage (0..4) ==========\n",
      "Accuracy:  0.5116\n",
      "Precision (macro): 0.4272\n",
      "Recall (macro):    0.4475\n",
      "F1-score (macro):  0.4145\n",
      "\n",
      "Chi tiết theo lớp (0..4):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    0(No-DR)       0.81      0.49      0.61       627\n",
      "           1       0.07      0.29      0.11        63\n",
      "           2       0.54      0.56      0.55       448\n",
      "           3       0.24      0.29      0.26        24\n",
      "           4       0.48      0.60      0.53        91\n",
      "\n",
      "    accuracy                           0.51      1253\n",
      "   macro avg       0.43      0.45      0.41      1253\n",
      "weighted avg       0.64      0.51      0.55      1253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======== EVALUATION ON TEST SET ========\n",
    "import os, numpy as np, tensorflow as tf\n",
    "from tensorflow.keras.applications.nasnet import preprocess_input as nasnet_preprocess\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# ---- cấu hình cơ bản ----\n",
    "TEST_DIR = r\"D:\\Diux\\\\hoctap\\DoAn\\ddr\\\\test_preprocess\"   \n",
    "THR = 0.5      \n",
    "IMG_SIZE  = 331        \n",
    "SEED = 42\n",
    "              \n",
    "BATCH     = 8                                        \n",
    "OUT_DIR  = r\"D:\\Diux\\\\hoctap\\DoAn\\\\models\\\\outputs_two_stage\"    \n",
    "# Đường dẫn model đã lưu \n",
    "M1_PATH = os.path.join(OUT_DIR, \"module1_ft_best.keras\")\n",
    "M2_PATH = os.path.join(OUT_DIR, \"module2_ft_best.keras\")\n",
    "\n",
    "assert os.path.exists(M1_PATH), f\"Không thấy model1 tại: {M1_PATH}\"\n",
    "assert os.path.exists(M2_PATH), f\"Không thấy model2 tại: {M2_PATH}\"\n",
    "\n",
    "m1 = tf.keras.models.load_model(M1_PATH, compile=False)\n",
    "m2 = tf.keras.models.load_model(M2_PATH, compile=False)\n",
    "\n",
    "# ---- dataset test ----\n",
    "def make_test_ds(test_dir, img_size=IMG_SIZE, batch=BATCH):\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        class_names=['0','1','2','3','4'],\n",
    "        image_size=(img_size, img_size),\n",
    "        batch_size=batch,\n",
    "        shuffle=False,\n",
    "        seed=SEED\n",
    "    )\n",
    "    # cast & preprocess theo NASNet\n",
    "    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(lambda x,y: (nasnet_preprocess(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = make_test_ds(TEST_DIR)\n",
    "\n",
    "# ---- gom toàn bộ X, y (để tiện route 2-stage) ----\n",
    "X_list, y_list = [], []\n",
    "for xb, yb in test_ds:\n",
    "    X_list.append(xb.numpy())\n",
    "    y_list.append(yb.numpy())\n",
    "X = np.concatenate(X_list, axis=0)          # (N, H, W, 3)\n",
    "y_true_5 = np.concatenate(y_list, axis=0)   # (N,), giá trị 0..4\n",
    "N = len(y_true_5)\n",
    "print(f\"Loaded TEST set: {N} samples\")\n",
    "\n",
    "# =========================================================\n",
    "# (1) ĐÁNH GIÁ MODULE 1: No-DR (0) vs DR (1)\n",
    "# =========================================================\n",
    "# y_true_bin: 0 nếu y==0, 1 nếu y in {1,2,3,4}\n",
    "y_true_bin = (y_true_5 > 0).astype(int)\n",
    "\n",
    "# dự đoán: sigmoid -> pDR -> nhị phân theo THR\n",
    "p_dr = m1.predict(X, verbose=0).reshape(-1)            # (N,)\n",
    "y_pred_bin = (p_dr >= THR).astype(int)                 # 0: No-DR, 1: DR\n",
    "\n",
    "acc_bin = accuracy_score(y_true_bin, y_pred_bin)\n",
    "prec_bin, rec_bin, f1_bin, _ = precision_recall_fscore_support(\n",
    "    y_true_bin, y_pred_bin, average='binary', zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n========== Module 1 (No-DR vs DR) ==========\")\n",
    "print(f\"Accuracy:  {acc_bin:.4f}\")\n",
    "print(f\"Precision: {prec_bin:.4f}\")\n",
    "print(f\"Recall:    {rec_bin:.4f}\")\n",
    "print(f\"F1-score:  {f1_bin:.4f}\")\n",
    "print(\"\\nChi tiết theo lớp:\")\n",
    "print(classification_report(y_true_bin, y_pred_bin, target_names=[\"No-DR(0)\", \"DR(1)\"], zero_division=0))\n",
    "\n",
    "# =========================================================\n",
    "# (2) ĐÁNH GIÁ MODULE 2: 4 LỚP (1..4 -> 0..3)\n",
    "#     Chỉ đánh giá trên các mẫu DR (y in {1,2,3,4})\n",
    "# =========================================================\n",
    "dr_idx = np.where(y_true_5 > 0)[0]\n",
    "if len(dr_idx) == 0:\n",
    "    print(\"\\n========== Module 2 (Stages 1..4) ==========\")\n",
    "    print(\"Bộ test không có mẫu DR (1..4), bỏ qua đánh giá Module 2.\")\n",
    "else:\n",
    "    X_dr = X[dr_idx]\n",
    "    y_true_4 = (y_true_5[dr_idx] - 1).astype(int)   # 1..4 -> 0..3\n",
    "\n",
    "    probs_4 = m2.predict(X_dr, verbose=0)           # (M, 4)\n",
    "    y_pred_4 = probs_4.argmax(axis=1)               # 0..3\n",
    "\n",
    "    acc_4 = accuracy_score(y_true_4, y_pred_4)\n",
    "    # macro trung bình trên 4 lớp\n",
    "    prec_4, rec_4, f1_4, _ = precision_recall_fscore_support(\n",
    "        y_true_4, y_pred_4, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    print(\"\\n========== Module 2 (Stages 1..4) ==========\")\n",
    "    print(f\"Accuracy:  {acc_4:.4f}\")\n",
    "    print(f\"Precision (macro): {prec_4:.4f}\")\n",
    "    print(f\"Recall (macro):    {rec_4:.4f}\")\n",
    "    print(f\"F1-score (macro):  {f1_4:.4f}\")\n",
    "    print(\"\\nChi tiết theo lớp (0..3 tương ứng 1..4):\")\n",
    "    print(classification_report(\n",
    "        y_true_4, y_pred_4,\n",
    "        target_names=[\"stage1(=1)\", \"stage2(=2)\", \"stage3(=3)\", \"stage4(=4)\"],\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "# =========================================================\n",
    "# (3) ĐÁNH GIÁ KẾT HỢP 2-STAGE TRÊN TOÀN BỘ 5 LỚP (0..4)\n",
    "#     - Nếu Module 1 dự đoán No-DR (0) -> nhãn 0\n",
    "#     - Nếu Module 1 dự đoán DR       -> chuyển Module 2 để phân 1..4\n",
    "# =========================================================\n",
    "y_pred_5 = np.zeros(N, dtype=int) \n",
    "dr_route_idx = np.where(y_pred_bin == 1)[0]\n",
    "\n",
    "if len(dr_route_idx) > 0:\n",
    "    X_for_m2 = X[dr_route_idx]\n",
    "    probs_m2 = m2.predict(X_for_m2, verbose=0)    # (K, 4)\n",
    "    cls_0_3 = probs_m2.argmax(axis=1)            # 0..3\n",
    "    y_pred_5[dr_route_idx] = cls_0_3 + 1         # map về 1..4\n",
    "\n",
    "acc_5 = accuracy_score(y_true_5, y_pred_5)\n",
    "prec_5, rec_5, f1_5, _ = precision_recall_fscore_support(\n",
    "    y_true_5, y_pred_5, average='macro', zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n========== Two-Stage (0..4) ==========\")\n",
    "print(f\"Accuracy:  {acc_5:.4f}\")\n",
    "print(f\"Precision (macro): {prec_5:.4f}\")\n",
    "print(f\"Recall (macro):    {rec_5:.4f}\")\n",
    "print(f\"F1-score (macro):  {f1_5:.4f}\")\n",
    "print(\"\\nChi tiết theo lớp (0..4):\")\n",
    "print(classification_report(\n",
    "    y_true_5, y_pred_5,\n",
    "    target_names=[\"0(No-DR)\", \"1\", \"2\", \"3\", \"4\"],\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1253 files belonging to 5 classes.\n",
      "[INFO] Loaded TEST set: 1253 samples\n",
      "\n",
      "========== Two-Stage Evaluation (0..4) ==========\n",
      "Accuracy:          0.5116\n",
      "Precision (macro): 0.4272\n",
      "Recall (macro):    0.4475\n",
      "F1-score (macro):  0.4145\n",
      "\n",
      "Chi tiết theo lớp (0..4):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    0(No-DR)       0.81      0.49      0.61       627\n",
      "           1       0.07      0.29      0.11        63\n",
      "           2       0.54      0.56      0.55       448\n",
      "           3       0.24      0.29      0.26        24\n",
      "           4       0.48      0.60      0.53        91\n",
      "\n",
      "    accuracy                           0.51      1253\n",
      "   macro avg       0.43      0.45      0.41      1253\n",
      "weighted avg       0.64      0.51      0.55      1253\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.nasnet import preprocess_input as nasnet_preprocess\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "THR = 0.5      \n",
    "IMG_SIZE  = 331        \n",
    "SEED = 42\n",
    "BATCH     = 8                                       \n",
    "OUT_DIR  = r\"D:\\Diux\\\\hoctap\\DoAn\\\\models\\\\outputs_two_stage\" \n",
    "TEST_DIR  = r\"D:\\Diux\\\\hoctap\\DoAn\\ddr\\\\test_preprocess\"  \n",
    "\n",
    "M1_PATH = os.path.join(OUT_DIR, \"module1_ft_best.keras\")\n",
    "M2_PATH = os.path.join(OUT_DIR, \"module2_ft_best.keras\")\n",
    "\n",
    "# ====== kiểm tra model ======\n",
    "assert os.path.exists(M1_PATH), f\"Không thấy model1: {M1_PATH}\"\n",
    "assert os.path.exists(M2_PATH), f\"Không thấy model2: {M2_PATH}\"\n",
    "\n",
    "m1 = tf.keras.models.load_model(M1_PATH, compile=False)\n",
    "m2 = tf.keras.models.load_model(M2_PATH, compile=False)\n",
    "\n",
    "# ====== dataset test ======\n",
    "def make_test_ds(test_dir, img_size=IMG_SIZE, batch=BATCH):\n",
    "    ds = tf.keras.utils.image_dataset_from_directory(\n",
    "        test_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"int\",\n",
    "        class_names=['0','1','2','3','4'],\n",
    "        image_size=(img_size, img_size),\n",
    "        batch_size=batch,\n",
    "        shuffle=False,\n",
    "        seed=SEED\n",
    "    )\n",
    "    ds = ds.map(lambda x,y: (tf.cast(x, tf.float32), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.map(lambda x,y: (nasnet_preprocess(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_ds = make_test_ds(TEST_DIR)\n",
    "\n",
    "# Gom toàn bộ X, y để route 2-stage\n",
    "X_list, y_list = [], []\n",
    "for xb, yb in test_ds:\n",
    "    X_list.append(xb.numpy())\n",
    "    y_list.append(yb.numpy())\n",
    "X = np.concatenate(X_list, axis=0)          # (N, H, W, 3)\n",
    "y_true = np.concatenate(y_list, axis=0)     # (N,), giá trị 0..4\n",
    "N = len(y_true)\n",
    "print(f\"[INFO] Loaded TEST set: {N} samples\")\n",
    "\n",
    "# ====== Two-Stage inference ======\n",
    "# Bước 1: Module 1 -> pDR\n",
    "p_dr = m1.predict(X, verbose=0).reshape(-1)\n",
    "y_pred_stage1 = (p_dr >= THR).astype(int)  # 0: No-DR, 1: DR\n",
    "\n",
    "# Bước 2: những mẫu DR -> Module 2\n",
    "y_pred_5 = np.zeros(N, dtype=int)          \n",
    "dr_idx = np.where(y_pred_stage1 == 1)[0]\n",
    "if len(dr_idx) > 0:\n",
    "    probs_m2 = m2.predict(X[dr_idx], verbose=0)   # (K, 4)\n",
    "    cls_0_3 = probs_m2.argmax(axis=1)            # 0..3\n",
    "    y_pred_5[dr_idx] = cls_0_3 + 1               # 1..4\n",
    "\n",
    "# ====== Metrics ======\n",
    "acc = accuracy_score(y_true, y_pred_5)\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred_5, average='macro', zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n========== Two-Stage Evaluation (0..4) ==========\")\n",
    "print(f\"Accuracy:          {acc:.4f}\")\n",
    "print(f\"Precision (macro): {prec:.4f}\")\n",
    "print(f\"Recall (macro):    {rec:.4f}\")\n",
    "print(f\"F1-score (macro):  {f1:.4f}\")\n",
    "\n",
    "print(\"\\nChi tiết theo lớp (0..4):\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred_5,\n",
    "    target_names=[\"0(No-DR)\", \"1\", \"2\", \"3\", \"4\"],\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ipynb-py-convert structure_model_nasnet_v2.ipynb structure_model_nasnet_v2.py"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8070203,
     "sourceId": 12765953,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
