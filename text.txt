Dau ra cua effb4_dualhead_stage1.keras

=== Stage 1: Train head (backbone frozen) ===
Epoch 1/20
 [1m    1/12518 [0m  [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m  [1m127:34:18 [0m 37s/step - loss: 4.2973 - ordinal_auc: 0.0781 - ordinal_loss: 1.1959 - softmax_acc: 0.0000e+00 - softmax_loss: 3.6993I0000 00:00:1754840052.531912   19203 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
 [1m12517/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37mâ” [0m  [1m0s [0m 103ms/step - loss: 1.5604 - ordinal_auc: 0.7778 - ordinal_loss: 0.4685 - softmax_acc: 0.5241 - softmax_loss: 1.32612025-08-10 11:55:46.560913: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
2025-08-10 11:55:46.704068: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m0s [0m 105ms/step - loss: 1.5604 - ordinal_auc: 0.7778 - ordinal_loss: 0.4685 - softmax_acc: 0.5241 - softmax_loss: 1.32612025-08-10 11:58:44.777093: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
2025-08-10 11:58:44.922975: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1519s [0m 118ms/step - loss: 1.5603 - ordinal_auc: 0.7778 - ordinal_loss: 0.4685 - softmax_acc: 0.5241 - softmax_loss: 1.3261 - val_loss: 1.1843 - val_ordinal_auc: 0.9094 - val_ordinal_loss: 0.3570 - val_softmax_acc: 0.5902 - val_softmax_loss: 1.0057 - learning_rate: 3.0000e-04
Epoch 2/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m0s [0m 103ms/step - loss: 1.1599 - ordinal_auc: 0.8746 - ordinal_loss: 0.3373 - softmax_acc: 0.6173 - softmax_loss: 0.99132025-08-10 12:20:23.103887: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 33554816 bytes after encountering the first element of size 33554816 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1442s [0m 115ms/step - loss: 1.1599 - ordinal_auc: 0.8746 - ordinal_loss: 0.3373 - softmax_acc: 0.6173 - softmax_loss: 0.9913 - val_loss: 1.1422 - val_ordinal_auc: 0.9106 - val_ordinal_loss: 0.3411 - val_softmax_acc: 0.6023 - val_softmax_loss: 0.9716 - learning_rate: 3.0000e-04
Epoch 3/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1441s [0m 115ms/step - loss: 1.1190 - ordinal_auc: 0.8838 - ordinal_loss: 0.3255 - softmax_acc: 0.6314 - softmax_loss: 0.9563 - val_loss: 0.9919 - val_ordinal_auc: 0.9189 - val_ordinal_loss: 0.2909 - val_softmax_acc: 0.6839 - val_softmax_loss: 0.8464 - learning_rate: 3.0000e-04
Epoch 4/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m0s [0m 103ms/step - loss: 1.0834 - ordinal_auc: 0.8909 - ordinal_loss: 0.3160 - softmax_acc: 0.6430 - softmax_loss: 0.92532025-08-10 13:08:25.356539: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 33554816 bytes after encountering the first element of size 33554816 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1442s [0m 115ms/step - loss: 1.0834 - ordinal_auc: 0.8909 - ordinal_loss: 0.3160 - softmax_acc: 0.6430 - softmax_loss: 0.9253 - val_loss: 0.9824 - val_ordinal_auc: 0.9203 - val_ordinal_loss: 0.2913 - val_softmax_acc: 0.6869 - val_softmax_loss: 0.8367 - learning_rate: 3.0000e-04
Epoch 5/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1440s [0m 115ms/step - loss: 1.0634 - ordinal_auc: 0.8958 - ordinal_loss: 0.3103 - softmax_acc: 0.6512 - softmax_loss: 0.9082 - val_loss: 0.9478 - val_ordinal_auc: 0.9224 - val_ordinal_loss: 0.2792 - val_softmax_acc: 0.6984 - val_softmax_loss: 0.8082 - learning_rate: 3.0000e-04
Epoch 6/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1440s [0m 115ms/step - loss: 1.0457 - ordinal_auc: 0.8974 - ordinal_loss: 0.3069 - softmax_acc: 0.6574 - softmax_loss: 0.8923 - val_loss: 0.9785 - val_ordinal_auc: 0.9216 - val_ordinal_loss: 0.2843 - val_softmax_acc: 0.6851 - val_softmax_loss: 0.8364 - learning_rate: 3.0000e-04
Epoch 7/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1445s [0m 115ms/step - loss: 1.0316 - ordinal_auc: 0.9006 - ordinal_loss: 0.3027 - softmax_acc: 0.6649 - softmax_loss: 0.8802 - val_loss: 0.9484 - val_ordinal_auc: 0.9232 - val_ordinal_loss: 0.2789 - val_softmax_acc: 0.7036 - val_softmax_loss: 0.8090 - learning_rate: 3.0000e-04
Epoch 8/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1446s [0m 116ms/step - loss: 1.0166 - ordinal_auc: 0.9033 - ordinal_loss: 0.2982 - softmax_acc: 0.6692 - softmax_loss: 0.8675 - val_loss: 0.9126 - val_ordinal_auc: 0.9257 - val_ordinal_loss: 0.2701 - val_softmax_acc: 0.7128 - val_softmax_loss: 0.7776 - learning_rate: 3.0000e-04
Epoch 9/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1445s [0m 115ms/step - loss: 1.0053 - ordinal_auc: 0.9056 - ordinal_loss: 0.2952 - softmax_acc: 0.6716 - softmax_loss: 0.8577 - val_loss: 0.9359 - val_ordinal_auc: 0.9233 - val_ordinal_loss: 0.2775 - val_softmax_acc: 0.7117 - val_softmax_loss: 0.7972 - learning_rate: 3.0000e-04
Epoch 10/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1445s [0m 115ms/step - loss: 0.9969 - ordinal_auc: 0.9073 - ordinal_loss: 0.2940 - softmax_acc: 0.6758 - softmax_loss: 0.8499 - val_loss: 0.9313 - val_ordinal_auc: 0.9254 - val_ordinal_loss: 0.2772 - val_softmax_acc: 0.7113 - val_softmax_loss: 0.7928 - learning_rate: 3.0000e-04
Epoch 11/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1445s [0m 115ms/step - loss: 0.9896 - ordinal_auc: 0.9085 - ordinal_loss: 0.2913 - softmax_acc: 0.6785 - softmax_loss: 0.8439 - val_loss: 0.9180 - val_ordinal_auc: 0.9261 - val_ordinal_loss: 0.2705 - val_softmax_acc: 0.7177 - val_softmax_loss: 0.7828 - learning_rate: 3.0000e-04
Epoch 12/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1445s [0m 115ms/step - loss: 0.9603 - ordinal_auc: 0.9135 - ordinal_loss: 0.2837 - softmax_acc: 0.6883 - softmax_loss: 0.8185 - val_loss: 0.9364 - val_ordinal_auc: 0.9275 - val_ordinal_loss: 0.2737 - val_softmax_acc: 0.7079 - val_softmax_loss: 0.7996 - learning_rate: 1.5000e-04
Epoch 13/20
 [1m12518/12518 [0m  [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” [0m [37m [0m  [1m1445s [0m 115ms/step - loss: 0.9500 - ordinal_auc: 0.9143 - ordinal_loss: 0.2825 - softmax_acc: 0.6927 - softmax_loss: 0.8088 - val_loss: 0.9162 - val_ordinal_auc: 0.9257 - val_ordinal_loss: 0.2720 - val_softmax_acc: 0.7169 - val_softmax_loss: 0.7802 - learning_rate: 1.5000e-04








=====================day ra chua apos 2019 state 1
=== Stage 1: Train head (backbone frozen) ===
Epoch 1/20
2025-08-11 09:04:50.305762: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 33554816 bytes after encountering the first element of size 33554816 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1754917490.333499   22258 service.cc:152] XLA service 0x77f3f8003160 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
I0000 00:00:1754917490.333522   22258 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6
2025-08-11 09:04:51.170630: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
I0000 00:00:1754917494.039541   22258 cuda_dnn.cc:529] Loaded cuDNN version 90300
2025-08-11 09:04:59.360979: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
2025-08-11 09:04:59.508901: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
[1m  1/367[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3:43:47[0m 37s/step - loss: 2.6409 - ordinal_auc: 0.1652 - ordinal_loss: 1.4009 - softmax_acc: 0.2500 - softmax_loss: 1.9405I0000 00:00:1754917515.554548   22258 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
[1m366/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 101ms/step - loss: 1.6732 - ordinal_auc: 0.7221 - ordinal_loss: 0.6643 - softmax_acc: 0.5945 - softmax_loss: 1.34112025-08-11 09:05:59.156021: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
2025-08-11 09:05:59.287826: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m115s[0m 215ms/step - loss: 1.6721 - ordinal_auc: 0.7224 - ordinal_loss: 0.6636 - softmax_acc: 0.5948 - softmax_loss: 1.3403 - val_loss: 0.8454 - val_ordinal_auc: 0.9202 - val_ordinal_loss: 0.3016 - val_softmax_acc: 0.7305 - val_softmax_loss: 0.7006 - learning_rate: 3.0000e-04
Epoch 2/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 129ms/step - loss: 1.1940 - ordinal_auc: 0.8214 - ordinal_loss: 0.4153 - softmax_acc: 0.6936 - softmax_loss: 0.9863 - val_loss: 0.7166 - val_ordinal_auc: 0.9357 - val_ordinal_loss: 0.2219 - val_softmax_acc: 0.7989 - val_softmax_loss: 0.6096 - learning_rate: 3.0000e-04
Epoch 3/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m48s[0m 130ms/step - loss: 0.9942 - ordinal_auc: 0.8480 - ordinal_loss: 0.3350 - softmax_acc: 0.7297 - softmax_loss: 0.8267 - val_loss: 0.6777 - val_ordinal_auc: 0.9433 - val_ordinal_loss: 0.1883 - val_softmax_acc: 0.8003 - val_softmax_loss: 0.5912 - learning_rate: 3.0000e-04
Epoch 4/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 127ms/step - loss: 0.8807 - ordinal_auc: 0.8844 - ordinal_loss: 0.2753 - softmax_acc: 0.7398 - softmax_loss: 0.7430 - val_loss: 0.6931 - val_ordinal_auc: 0.9457 - val_ordinal_loss: 0.1834 - val_softmax_acc: 0.7907 - val_softmax_loss: 0.6054 - learning_rate: 3.0000e-04
Epoch 5/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m48s[0m 130ms/step - loss: 0.8836 - ordinal_auc: 0.8885 - ordinal_loss: 0.2686 - softmax_acc: 0.7484 - softmax_loss: 0.7493 - val_loss: 0.6539 - val_ordinal_auc: 0.9491 - val_ordinal_loss: 0.1683 - val_softmax_acc: 0.7934 - val_softmax_loss: 0.5726 - learning_rate: 3.0000e-04
Epoch 6/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 129ms/step - loss: 0.8453 - ordinal_auc: 0.9047 - ordinal_loss: 0.2509 - softmax_acc: 0.7509 - softmax_loss: 0.7199 - val_loss: 0.6406 - val_ordinal_auc: 0.9466 - val_ordinal_loss: 0.1641 - val_softmax_acc: 0.8167 - val_softmax_loss: 0.5682 - learning_rate: 3.0000e-04
Epoch 7/20
[1m  2/367[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m36s[0m 101ms/step - loss: 0.9377 - ordinal_auc: 0.3594 - ordinal_loss: 0.2405 - softmax_acc: 0.7188 - softmax_loss: 0.81752025-08-11 09:10:31.010758: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 33554816 bytes after encountering the first element of size 33554816 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 127ms/step - loss: 0.8139 - ordinal_auc: 0.9073 - ordinal_loss: 0.2448 - softmax_acc: 0.7523 - softmax_loss: 0.6915 - val_loss: 0.6515 - val_ordinal_auc: 0.9479 - val_ordinal_loss: 0.1628 - val_softmax_acc: 0.7989 - val_softmax_loss: 0.5735 - learning_rate: 3.0000e-04
Epoch 8/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 128ms/step - loss: 0.7948 - ordinal_auc: 0.9066 - ordinal_loss: 0.2493 - softmax_acc: 0.7641 - softmax_loss: 0.6701 - val_loss: 0.6607 - val_ordinal_auc: 0.9518 - val_ordinal_loss: 0.1605 - val_softmax_acc: 0.8030 - val_softmax_loss: 0.5881 - learning_rate: 3.0000e-04
Epoch 9/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m48s[0m 131ms/step - loss: 0.7295 - ordinal_auc: 0.9215 - ordinal_loss: 0.2261 - softmax_acc: 0.7619 - softmax_loss: 0.6164 - val_loss: 0.6218 - val_ordinal_auc: 0.9544 - val_ordinal_loss: 0.1533 - val_softmax_acc: 0.8140 - val_softmax_loss: 0.5572 - learning_rate: 3.0000e-04
Epoch 10/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 128ms/step - loss: 0.7324 - ordinal_auc: 0.9240 - ordinal_loss: 0.2214 - softmax_acc: 0.7651 - softmax_loss: 0.6218 - val_loss: 0.6357 - val_ordinal_auc: 0.9546 - val_ordinal_loss: 0.1621 - val_softmax_acc: 0.8085 - val_softmax_loss: 0.5637 - learning_rate: 3.0000e-04
Epoch 11/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 128ms/step - loss: 0.6902 - ordinal_auc: 0.9337 - ordinal_loss: 0.2106 - softmax_acc: 0.7817 - softmax_loss: 0.5849 - val_loss: 0.6338 - val_ordinal_auc: 0.9534 - val_ordinal_loss: 0.1560 - val_softmax_acc: 0.7948 - val_softmax_loss: 0.5636 - learning_rate: 3.0000e-04
Epoch 12/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 128ms/step - loss: 0.7298 - ordinal_auc: 0.9323 - ordinal_loss: 0.2223 - softmax_acc: 0.7832 - softmax_loss: 0.6187 - val_loss: 0.6250 - val_ordinal_auc: 0.9552 - val_ordinal_loss: 0.1534 - val_softmax_acc: 0.8112 - val_softmax_loss: 0.5550 - learning_rate: 3.0000e-04
Epoch 13/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m48s[0m 131ms/step - loss: 0.6319 - ordinal_auc: 0.9459 - ordinal_loss: 0.1966 - softmax_acc: 0.8027 - softmax_loss: 0.5336 - val_loss: 0.5873 - val_ordinal_auc: 0.9556 - val_ordinal_loss: 0.1514 - val_softmax_acc: 0.8140 - val_softmax_loss: 0.5194 - learning_rate: 1.5000e-04
Epoch 14/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m48s[0m 131ms/step - loss: 0.6553 - ordinal_auc: 0.9383 - ordinal_loss: 0.2025 - softmax_acc: 0.7904 - softmax_loss: 0.5540 - val_loss: 0.5871 - val_ordinal_auc: 0.9560 - val_ordinal_loss: 0.1546 - val_softmax_acc: 0.8276 - val_softmax_loss: 0.5180 - learning_rate: 1.5000e-04
Epoch 15/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m48s[0m 131ms/step - loss: 0.6224 - ordinal_auc: 0.9514 - ordinal_loss: 0.1932 - softmax_acc: 0.8080 - softmax_loss: 0.5258 - val_loss: 0.5819 - val_ordinal_auc: 0.9551 - val_ordinal_loss: 0.1516 - val_softmax_acc: 0.8276 - val_softmax_loss: 0.5133 - learning_rate: 1.5000e-04
Epoch 16/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m48s[0m 131ms/step - loss: 0.6620 - ordinal_auc: 0.9453 - ordinal_loss: 0.2101 - softmax_acc: 0.7887 - softmax_loss: 0.5569 - val_loss: 0.5791 - val_ordinal_auc: 0.9550 - val_ordinal_loss: 0.1515 - val_softmax_acc: 0.8249 - val_softmax_loss: 0.5096 - learning_rate: 1.5000e-04
Epoch 17/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 127ms/step - loss: 0.6086 - ordinal_auc: 0.9493 - ordinal_loss: 0.1968 - softmax_acc: 0.8111 - softmax_loss: 0.5102 - val_loss: 0.5966 - val_ordinal_auc: 0.9533 - val_ordinal_loss: 0.1548 - val_softmax_acc: 0.8112 - val_softmax_loss: 0.5271 - learning_rate: 1.5000e-04
Epoch 18/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 127ms/step - loss: 0.5738 - ordinal_auc: 0.9487 - ordinal_loss: 0.1868 - softmax_acc: 0.8188 - softmax_loss: 0.4804 - val_loss: 0.6106 - val_ordinal_auc: 0.9469 - val_ordinal_loss: 0.1658 - val_softmax_acc: 0.8167 - val_softmax_loss: 0.5432 - learning_rate: 1.5000e-04
Epoch 19/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 128ms/step - loss: 0.5748 - ordinal_auc: 0.9419 - ordinal_loss: 0.1866 - softmax_acc: 0.8303 - softmax_loss: 0.4815 - val_loss: 0.5874 - val_ordinal_auc: 0.9572 - val_ordinal_loss: 0.1506 - val_softmax_acc: 0.8181 - val_softmax_loss: 0.5145 - learning_rate: 1.5000e-04
Epoch 20/20
[1m367/367[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m47s[0m 128ms/step - loss: 0.6134 - ordinal_auc: 0.9499 - ordinal_loss: 0.1914 - softmax_acc: 0.8004 - softmax_loss: 0.5177 - val_loss: 0.5824 - val_ordinal_auc: 0.9545 - val_ordinal_loss: 0.1511 - val_softmax_acc: 0.8222 - val_softmax_loss: 0.5157 - learning_rate: 7.5000e-05


==============================

thu mux archive test chua tien xu ly
====== MAIN METRICS (Fused) ======
Quadratic Weighted Kappa : 0.49166
Accuracy                 : 0.51912
Macro F1                 : 0.35821

Classification report (per-class):
              precision    recall  f1-score   support

           0     0.6477    0.8182    0.7230      6896
           1     0.2281    0.4753    0.3083      1862
           2     0.3723    0.0287    0.0533      2999
           3     0.4154    0.1431    0.2129       978
           4     0.5940    0.4222    0.4936      1466

    accuracy                         0.5191     14201
   macro avg     0.4515    0.3775    0.3582     14201
weighted avg     0.5130    0.5191    0.4684     14201


====== CONFUSION MATRIX ======
(HÃ ng: nhÃ£n tháº­t, Cá»™t: nhÃ£n dá»± Ä‘oÃ¡n)
[[5642 1181   14   15   44]
 [ 902  885   21   14   40]
 [1623 1032   86   96  162]
 [ 182  428   51  140  177]
 [ 362  354   59   72  619]]



Thu muc archive tesst da tien xu ly
====== MAIN METRICS (Fused) ======
Quadratic Weighted Kappa : 0.74223
Accuracy                 : 0.61665
Macro F1                 : 0.45137

Classification report (per-class):
              precision    recall  f1-score   support

           0     0.7498    0.9198    0.8261      6896
           1     0.2926    0.1799    0.2228      1862
           2     0.4179    0.2571    0.3183      2999
           3     0.3508    0.2393    0.2845       978
           4     0.5154    0.7326    0.6051      1466

    accuracy                         0.6166     14201
   macro avg     0.4653    0.4657    0.4514     14201
weighted avg     0.5681    0.6166    0.5797     14201


====== CONFUSION MATRIX ======
(HÃ ng: nhÃ£n tháº­t, Cá»™t: nhÃ£n dá»± Ä‘oÃ¡n)
[[6343   73  373   17   90]
 [1112  335  336    9   70]
 [ 901  590  771  291  446]
 [  56  106  178  234  404]
 [  48   41  187  116 1074]]



Thu muc archive val
 ====== MAIN METRICS (Fused) ======
Quadratic Weighted Kappa : 0.75498
Accuracy                 : 0.71454
Macro F1                 : 0.61558

Classification report (per-class):
              precision    recall  f1-score   support

           0     0.7873    0.9106    0.8445      6253
           1     0.7004    0.4184    0.5239      1587
           2     0.5559    0.5525    0.5541      2612
           3     0.5542    0.4436    0.4928       807
           4     0.7170    0.6159    0.6626      1156

    accuracy                         0.7145     12415
   macro avg     0.6630    0.5882    0.6156     12415
weighted avg     0.7058    0.7145    0.7026     12415


====== CONFUSION MATRIX ======
(HÃ ng: nhÃ£n tháº­t, Cá»™t: nhÃ£n dá»± Ä‘oÃ¡n)
[[5694   93  430    7   29]
 [ 593  664  272   15   43]
 [ 870   78 1443  132   89]
 [  32   47  250  358  120]
 [  43   66  201  134  712]]






 ====== thu muc archive test dung model apos19 (Fused) ======
Quadratic Weighted Kappa : 0.53820
Accuracy                 : 0.54574
Macro F1                 : 0.32915

Classification report (per-class):
              precision    recall  f1-score   support

           0     0.6178    0.9262    0.7412      6896
           1     0.1385    0.0918    0.1104      1862
           2     0.3383    0.1911    0.2442      2999
           3     0.6053    0.0235    0.0453       978
           4     0.6652    0.4065    0.5047      1466

    accuracy                         0.5457     14201
   macro avg     0.4730    0.3278    0.3292     14201
weighted avg     0.5000    0.5457    0.4812     14201


====== CONFUSION MATRIX ======
(HÃ ng: nhÃ£n tháº­t, Cá»™t: nhÃ£n dá»± Ä‘oÃ¡n)
[[6387  231  203    3   72]
 [1635  171   48    0    8]
 [1853  480  573    5   88]
 [ 209  183  431   23  132]
 [ 254  170  439    7  596]]







model eyepac state 1 , dir apos19
 ====== MAIN METRICS (Fused) ======
Quadratic Weighted Kappa : 0.75603
Accuracy                 : 0.44558
Macro F1                 : 0.33715

Classification report (per-class):
              precision    recall  f1-score   support

           0     0.9623    0.6711    0.7907      1444
           1     0.0672    0.0811    0.0735       296
           2     0.2861    0.1363    0.1846       800
           3     0.1225    0.8581    0.2143       155
           4     0.7100    0.3008    0.4226       236

    accuracy                         0.4456      2931
   macro avg     0.4296    0.4095    0.3372      2931
weighted avg     0.6226    0.4456    0.4927      2931


====== CONFUSION MATRIX ======
(HÃ ng: nhÃ£n tháº­t, Cá»™t: nhÃ£n dá»± Ä‘oÃ¡n)
[[969 318 154   3   0]
 [ 27  24  98 142   5]
 [ 10  11 109 662   8]
 [  0   0   6 133  16]
 [  1   4  14 146  71]]








 == thu muc val , model apos 2019 state 2
 ====== MAIN METRICS (Fused) ======
Quadratic Weighted Kappa : 0.90752
Accuracy                 : 0.85226
Macro F1                 : 0.69109

Classification report (per-class):
              precision    recall  f1-score   support

           0     0.9809    0.9945    0.9876       361
           1     0.7286    0.6892    0.7083        74
           2     0.7623    0.8543    0.8057       199
           3     0.5000    0.2895    0.3667        38
           4     0.6400    0.5424    0.5872        59

    accuracy                         0.8523       731
   macro avg     0.7224    0.6740    0.6911       731
weighted avg     0.8433    0.8523    0.8452       731


====== CONFUSION MATRIX ======
(HÃ ng: nhÃ£n tháº­t, Cá»™t: nhÃ£n dá»± Ä‘oÃ¡n)
[[359   2   0   0   0]
 [  4  51  17   2   0]
 [  3  15 170   6   5]
 [  0   1  13  11  13]
 [  0   1  23   3  32]]








test dir: apos19 , model test : effb4_eyespacs2_dualhead_stage2
 ====== MAIN METRICS (Fused) ======
Quadratic Weighted Kappa : 0.87978
Accuracy                 : 0.70041
Macro F1                 : 0.46411

Classification report (per-class):
              precision    recall  f1-score   support

           0     0.9593    0.9806    0.9699       361
           1     0.0000    0.0000    0.0000        74
           2     0.6043    0.5678    0.5855       199
           3     0.1439    0.5000    0.2235        38
           4     0.7027    0.4407    0.5417        59

    accuracy                         0.7004       731
   macro avg     0.4821    0.4978    0.4641       731
weighted avg     0.7025    0.7004    0.6937       731


====== CONFUSION MATRIX ======
(HÃ ng: nhÃ£n tháº­t, Cá»™t: nhÃ£n dá»± Ä‘oÃ¡n)
[[354   6   1   0   0]
 [ 11   0  54   9   0]
 [  2   0 113  84   0]
 [  1   0   7  19  11]
 [  1   0  12  20  26]]


 